# 3. Kurulum ve KonfigÃ¼rasyon

## 3.1 Sistem Gereksinimleri

### ğŸ’» DonanÄ±m Gereksinimleri

#### Minimum Sistem Gereksinimleri
| BileÅŸen | Minimum | Ã–nerilen | Optimal |
|---------|---------|----------|---------|
| **Ä°ÅŸlemci** | Intel i3 / AMD Ryzen 3 | Intel i5 / AMD Ryzen 5 | Intel i7 / AMD Ryzen 7+ |
| **RAM** | 4GB | 8GB | 16GB+ |
| **Disk AlanÄ±** | 2GB boÅŸ alan | 5GB boÅŸ alan | 10GB+ boÅŸ alan |
| **GPU** | Ä°steÄŸe baÄŸlÄ± | 4GB VRAM | 8GB+ VRAM (yerel LLM iÃ§in) |
| **Ä°nternet** | 1 Mbps | 10 Mbps | 50 Mbps+ |

#### Platform DesteÄŸi
```bash
# Desteklenen iÅŸletim sistemleri
âœ… Windows 10/11 (x64)
âœ… macOS 10.15+ (Intel/Apple Silicon)
âœ… Linux Ubuntu 18.04+ / Debian 10+
âœ… Linux CentOS 7+ / RHEL 7+
âœ… Linux Arch / Manjaro
```

### ğŸ› ï¸ YazÄ±lÄ±m Gereksinimleri

#### Temel Gereksinimler
```json
{
  "vscode": {
    "minimum": "1.90.0",
    "recommended": "1.95.0+",
    "features": [
      "Webview API",
      "Extension API",
      "Command API",
      "File System API"
    ]
  },
  "nodejs": {
    "minimum": "18.0.0",
    "recommended": "20.0.0+",
    "lts": "20.11.0"
  },
  "npm": {
    "minimum": "9.0.0",
    "recommended": "10.0.0+"
  }
}
```

#### Opsiyonel BaÄŸÄ±mlÄ±lÄ±klar
```bash
# Python (vLLM iÃ§in)
Python 3.8+
pip 21.0+
virtualenv

# Git (versiyon kontrolÃ¼ iÃ§in)
Git 2.25+

# Docker (containerized deployment iÃ§in)
Docker 20.10+
Docker Compose 2.0+
```

---

## 3.2 Extension Kurulumu

### ğŸ“¦ VSIX Paketi ile Kurulum

#### 1. Manuel Kurulum
```bash
# VSIX dosyasÄ±nÄ± indirin
curl -L -o baykar-ivme-2.2.7.vsix \
  "https://github.com/baykar/ivme/releases/latest/download/baykar-ivme-2.2.7.vsix"

# Extension'Ä± yÃ¼kleyin
code --install-extension baykar-ivme-2.2.7.vsix

# Kurulumu doÄŸrulayÄ±n
code --list-extensions | grep ivme
```

#### 2. VS Code UI ile Kurulum
1. VS Code'u aÃ§Ä±n
2. `Ctrl+Shift+P` (Windows/Linux) veya `Cmd+Shift+P` (macOS)
3. "Extensions: Install from VSIX..." komutunu Ã§alÄ±ÅŸtÄ±rÄ±n
4. Ä°ndirilen `.vsix` dosyasÄ±nÄ± seÃ§in
5. "Reload" butonuna tÄ±klayÄ±n

#### 3. Komut SatÄ±rÄ± ile Toplu Kurulum
```bash
#!/bin/bash
# install-ivme.sh

VSIX_URL="https://github.com/baykar/ivme/releases/latest/download/baykar-ivme-2.2.7.vsix"
TEMP_FILE="/tmp/baykar-ivme.vsix"

echo "Ä°vme Extension kuruluyor..."

# VSIX dosyasÄ±nÄ± indir
curl -L -o "$TEMP_FILE" "$VSIX_URL"

# Extension'Ä± yÃ¼kle
code --install-extension "$TEMP_FILE"

# GeÃ§ici dosyayÄ± sil
rm "$TEMP_FILE"

echo "Kurulum tamamlandÄ±! VS Code'u yeniden baÅŸlatÄ±n."
```

### ğŸ”„ GÃ¼ncelleme Ä°ÅŸlemi

#### Otomatik GÃ¼ncelleme
```typescript
// Extension otomatik gÃ¼ncelleme kontrolÃ¼
interface UpdateChecker {
  checkForUpdates(): Promise<UpdateInfo | null>;
  downloadUpdate(updateInfo: UpdateInfo): Promise<void>;
  applyUpdate(): Promise<void>;
}

// Manuel gÃ¼ncelleme kontrolÃ¼
const updateChecker = new UpdateChecker();
const updateInfo = await updateChecker.checkForUpdates();

if (updateInfo) {
  vscode.window.showInformationMessage(
    `Ä°vme ${updateInfo.version} sÃ¼rÃ¼mÃ¼ mevcut. GÃ¼ncellemek ister misiniz?`,
    'GÃ¼ncelle', 'Daha Sonra'
  ).then(selection => {
    if (selection === 'GÃ¼ncelle') {
      updateChecker.downloadUpdate(updateInfo);
    }
  });
}
```

#### Manuel GÃ¼ncelleme
```bash
# Mevcut sÃ¼rÃ¼mÃ¼ kaldÄ±r
code --uninstall-extension ivme.ivme-ivme

# Yeni sÃ¼rÃ¼mÃ¼ yÃ¼kle  
code --install-extension baykar-ivme-2.2.7.vsix

# Extension listesini kontrol et
code --list-extensions --show-versions | grep ivme
```

---

## 3.3 AI Servisleri KonfigÃ¼rasyonu

### ğŸ¤– vLLM Sunucusu Kurulumu

#### Docker ile Kurulum (Ã–nerilen)
```yaml
# docker-compose.yml
version: '3.8'
services:
  vllm-server:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models
      - ./cache:/cache
    environment:
      - MODEL_NAME=Qwen/Qwen1.5-7B-Chat
      - HOST=0.0.0.0
      - PORT=8000
      - GPU_MEMORY_UTILIZATION=0.8
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      python -m vllm.entrypoints.openai.api_server
      --model ${MODEL_NAME}
      --host ${HOST}
      --port ${PORT}
      --gpu-memory-utilization ${GPU_MEMORY_UTILIZATION}
      --max-model-len 4096
      --enable-chunked-prefill
      --max-num-batched-tokens 8192
```

```bash
# Docker compose ile baÅŸlatma
docker-compose up -d vllm-server

# Log kontrolÃ¼
docker-compose logs -f vllm-server

# Health check
curl http://localhost:8000/v1/models
```

#### Manuel Python Kurulumu
```bash
# Python sanal ortamÄ± oluÅŸtur
python -m venv vllm-env
source vllm-env/bin/activate  # Linux/Mac
# vllm-env\Scripts\activate   # Windows

# vLLM ve baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼kle
pip install --upgrade pip
pip install vllm
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Model indirme (opsiyonel - otomatik indirilir)
python -c "
from huggingface_hub import snapshot_download
snapshot_download('Qwen/Qwen1.5-7B-Chat', cache_dir='./models')
"

# Sunucuyu baÅŸlat
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen1.5-7B-Chat \
    --host 0.0.0.0 \
    --port 8000 \
    --gpu-memory-utilization 0.8 \
    --max-model-len 4096
```

#### vLLM KonfigÃ¼rasyon DosyasÄ±
```json
// vllm-config.json
{
  "model": {
    "name": "Qwen/Qwen1.5-7B-Chat",
    "trust_remote_code": true,
    "max_model_len": 4096,
    "gpu_memory_utilization": 0.8,
    "tensor_parallel_size": 1
  },
  "server": {
    "host": "0.0.0.0",
    "port": 8000,
    "ssl_keyfile": null,
    "ssl_certfile": null,
    "root_path": null,
    "middleware": []
  },
  "engine": {
    "max_num_batched_tokens": 8192,
    "max_num_seqs": 256,
    "enable_chunked_prefill": true,
    "disable_log_stats": false
  }
}
```

### ğŸ’ Google Gemini KonfigÃ¼rasyonu

#### API AnahtarÄ± Alma
1. **Google AI Studio**'ya gidin: https://aistudio.google.com/
2. **Get API Key** butonuna tÄ±klayÄ±n
3. Yeni bir proje oluÅŸturun veya mevcut projeyi seÃ§in
4. API anahtarÄ±nÄ± kopyalayÄ±n

#### API AnahtarÄ± GÃ¼venlik
```typescript
// GÃ¼venli API anahtar saklama
class SecureApiKeyManager {
  private static readonly GEMINI_KEY = 'gemini.apiKey';
  
  static async storeApiKey(
    context: vscode.ExtensionContext, 
    apiKey: string
  ): Promise<void> {
    await context.secrets.store(this.GEMINI_KEY, apiKey);
  }
  
  static async getApiKey(
    context: vscode.ExtensionContext
  ): Promise<string | undefined> {
    return context.secrets.get(this.GEMINI_KEY);
  }
  
  static async deleteApiKey(
    context: vscode.ExtensionContext
  ): Promise<void> {
    await context.secrets.delete(this.GEMINI_KEY);
  }
}
```

#### Gemini Servis Testi
```bash
# API anahtarÄ±nÄ± test et
curl -H "Content-Type: application/json" \
     -d '{"contents":[{"parts":[{"text":"Hello"}]}]}' \
     "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=YOUR_API_KEY"
```

---

## 3.4 Extension AyarlarÄ±

### âš™ï¸ Ayar Kategorileri

#### 1. Servis AyarlarÄ±
```json
// settings.json Ã¶rneÄŸi
{
  "baykar-ai-fixer.api.activeService": "vLLM",
  "baykar-ai-fixer.vllm.baseUrl": "http://localhost:8000/v1",
  "baykar-ai-fixer.vllm.modelName": "Qwen/Qwen1.5-7B-Chat",
  "baykar-ai-fixer.vllm.embeddingModelName": "",
  "baykar-ai-fixer.gemini.apiKey": "your-api-key-here"
}
```

#### 2. Chat AyarlarÄ±
```json
{
  "baykar-ai-fixer.chat.conversationHistoryLimit": 2,
  "baykar-ai-fixer.chat.tokenLimit": 12000,
  "baykar-ai-fixer.chat.temperature": 0.7
}
```

#### 3. Ä°ndeksleme AyarlarÄ±
```json
{
  "baykar-ai-fixer.indexing.enabled": true,
  "baykar-ai-fixer.indexing.sourceName": "workspace",
  "baykar-ai-fixer.indexing.includeGlobs": [
    "**/*.ts", "**/*.tsx", "**/*.js", "**/*.jsx",
    "**/*.py", "**/*.{c,cc,cpp,cxx,h,hpp,hxx,c++}",
    "**/*.json", "**/*.css"
  ],
  "baykar-ai-fixer.indexing.excludeGlobs": [
    "**/node_modules/**", "**/dist/**", "**/out/**",
    "**/.git/**", "**/.ivme/**", "**/.vscode/**"
  ],
  "baykar-ai-fixer.indexing.summaryTimeoutMs": 25000,
  "baykar-ai-fixer.indexing.embeddingTimeoutMs": 45000
}
```

### ğŸ¨ UI AyarlarÄ±
```json
{
  "baykar-ai-fixer.ui.agentModeActive": false,
  "baykar-ai-fixer.ui.agentBarExpanded": false,
  "baykar-ai-fixer.ui.language": "tr",
  "baykar-ai-fixer.ui.theme": "auto",
  "baykar-ai-fixer.ui.backgroundVideo": true
}
```

### ğŸ”§ GeliÅŸmiÅŸ Ayarlar

#### Workspace-specific Ayarlar
```json
// .vscode/settings.json
{
  "baykar-ai-fixer.indexing.sourceName": "my-project-v1",
  "baykar-ai-fixer.indexing.vectorStorePath": "./.ivme/vector_store.json",
  "baykar-ai-fixer.indexing.includeGlobs": [
    "src/**/*.ts",
    "lib/**/*.js",
    "!**/*.test.*"
  ]
}
```

#### Global User AyarlarÄ±
```json
// ~/.vscode/User/settings.json
{
  "baykar-ai-fixer.api.activeService": "Gemini",
  "baykar-ai-fixer.chat.temperature": 0.5,
  "baykar-ai-fixer.ui.language": "en"
}
```

---

## 3.5 Ä°lk Kurulum SihirbazÄ±

### ğŸ§™â€â™‚ï¸ Setup Wizard AkÄ±ÅŸÄ±

```typescript
// Setup wizard implementation
class SetupWizard {
  private steps: WizardStep[] = [
    new WelcomeStep(),
    new ServiceSelectionStep(),
    new VllmConfigStep(),
    new GeminiConfigStep(),
    new IndexingSetupStep(),
    new CompletionStep()
  ];
  
  async run(): Promise<SetupResult> {
    const config = new ExtensionConfig();
    
    for (const step of this.steps) {
      const result = await step.execute(config);
      
      if (result.action === 'cancel') {
        return new SetupResult('cancelled');
      }
      
      if (result.action === 'back') {
        // Go back to previous step
        continue;
      }
      
      config.merge(result.config);
    }
    
    await this.applyConfiguration(config);
    return new SetupResult('completed', config);
  }
}

// Welcome step
class WelcomeStep implements WizardStep {
  async execute(config: ExtensionConfig): Promise<StepResult> {
    const selection = await vscode.window.showInformationMessage(
      'ğŸš€ Ä°vme Extension\'Ä±na hoÅŸ geldiniz! Kuruluma baÅŸlamak iÃ§in devam edin.',
      { modal: true },
      'Devam Et', 'Ä°ptal'
    );
    
    if (selection === 'Ä°ptal') {
      return new StepResult('cancel');
    }
    
    return new StepResult('next');
  }
}

// Service selection step
class ServiceSelectionStep implements WizardStep {
  async execute(config: ExtensionConfig): Promise<StepResult> {
    const items = [
      {
        label: 'ğŸ¤– vLLM (Yerel Sunucu)',
        description: 'HÄ±zlÄ± ve gÃ¼venli, yerel sunucu gerektirir',
        detail: 'Ã–nerilen: Daha iyi performans ve gizlilik',
        value: 'vLLM'
      },
      {
        label: 'ğŸ’ Google Gemini (Bulut)',
        description: 'GÃ¼Ã§lÃ¼ AI modelleri, internet baÄŸlantÄ±sÄ± gerektirir',
        detail: 'API anahtarÄ± gerekir',
        value: 'Gemini'
      }
    ];
    
    const selection = await vscode.window.showQuickPick(items, {
      title: 'AI Servisi SeÃ§in',
      placeHolder: 'Hangi AI servisini kullanmak istiyorsunuz?'
    });
    
    if (!selection) {
      return new StepResult('cancel');
    }
    
    config.set('api.activeService', selection.value);
    return new StepResult('next');
  }
}
```

### ğŸ“‹ Kurulum Kontrol Listesi

#### Pre-installation Checklist
```bash
# Sistem uyumluluÄŸu kontrolÃ¼
âœ… VS Code 1.90.0+ yÃ¼klÃ¼
âœ… Node.js 18+ yÃ¼klÃ¼  
âœ… Yeterli disk alanÄ± (2GB+)
âœ… Ä°nternet baÄŸlantÄ±sÄ±

# vLLM iÃ§in ek kontroller (eÄŸer seÃ§ilmiÅŸse)
âœ… Python 3.8+ yÃ¼klÃ¼
âœ… pip gÃ¼ncel (21.0+)
âœ… GPU sÃ¼rÃ¼cÃ¼leri (CUDA iÃ§in)
âœ… Yeterli RAM (8GB+)
```

#### Post-installation Verification
```typescript
// Installation verification
class InstallationVerifier {
  async verify(): Promise<VerificationResult> {
    const results = new VerificationResult();
    
    // Extension yÃ¼klÃ¼ mÃ¼?
    const extensions = vscode.extensions.all;
    const ivmeExtension = extensions.find(ext => 
      ext.id === 'ivme.ivme-ivme'
    );
    
    results.add('extension_installed', !!ivmeExtension);
    
    // API servisleri eriÅŸilebilir mi?
    if (this.isVllmConfigured()) {
      results.add('vllm_connection', await this.testVllmConnection());
    }
    
    if (this.isGeminiConfigured()) {
      results.add('gemini_connection', await this.testGeminiConnection());
    }
    
    // Workspace indexing Ã§alÄ±ÅŸÄ±yor mu?
    results.add('indexing_ready', await this.testIndexing());
    
    return results;
  }
  
  private async testVllmConnection(): Promise<boolean> {
    try {
      const config = vscode.workspace.getConfiguration('baykar-ai-fixer');
      const baseUrl = config.get<string>('vllm.baseUrl');
      
      const response = await fetch(`${baseUrl}/models`);
      return response.ok;
    } catch {
      return false;
    }
  }
}
```

---

## 3.6 Ã‡oklu Ortam KonfigÃ¼rasyonu

### ğŸ¢ Enterprise Deployment

#### Centralized Configuration
```yaml
# enterprise-config.yml
apiVersion: v1
kind: ConfigMap
metadata:
  name: ivme-config
data:
  default-settings.json: |
    {
      "baykar-ai-fixer.api.activeService": "vLLM",
      "baykar-ai-fixer.vllm.baseUrl": "http://internal-vllm.company.com:8000/v1",
      "baykar-ai-fixer.indexing.enabled": true,
      "baykar-ai-fixer.ui.language": "tr"
    }
  
  policy.json: |
    {
      "allowedServices": ["vLLM"],
      "blockedServices": ["Gemini"],
      "maxTokenLimit": 8000,
      "indexingPolicy": {
        "allowedFileTypes": ["ts", "js", "py"],
        "excludePaths": ["**/secret/**", "**/private/**"]
      }
    }
```

#### Group Policy Integration (Windows)
```registry
[HKEY_LOCAL_MACHINE\SOFTWARE\Policies\VSCode\Extensions\ivme-ivme]
"AllowedServices"=dword:00000001
"DefaultService"="vLLM"
"VllmBaseUrl"="http://company-vllm.internal:8000/v1"
"ForceSettings"=dword:00000001
```

### ğŸ³ Docker Development Environment

```dockerfile
# Dockerfile.dev
FROM mcr.microsoft.com/vscode/devcontainers/typescript-node:18

# vLLM dependencies
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-venv

# Create vLLM environment
RUN python3 -m venv /opt/vllm-env
RUN /opt/vllm-env/bin/pip install vllm

# Extension development tools
RUN npm install -g @vscode/vsce yo generator-code

# Copy extension source
COPY . /workspace
WORKDIR /workspace

# Install extension dependencies
RUN npm install

# Compile extension
RUN npm run compile

# Expose ports
EXPOSE 8000 3000

# Start script
COPY docker/start.sh /start.sh
RUN chmod +x /start.sh

CMD ["/start.sh"]
```

```bash
#!/bin/bash
# docker/start.sh

# Start vLLM server in background
/opt/vllm-env/bin/python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen1.5-7B-Chat \
    --host 0.0.0.0 \
    --port 8000 &

# Wait for vLLM to be ready
echo "Waiting for vLLM server..."
while ! curl -s http://localhost:8000/v1/models > /dev/null; do
    sleep 2
done

echo "vLLM server is ready!"

# Start VS Code server (if using code-server)
if command -v code-server &> /dev/null; then
    code-server --bind-addr 0.0.0.0:3000 /workspace
else
    echo "Development environment ready!"
    tail -f /dev/null
fi
```

### â˜ï¸ Cloud Configuration

#### AWS Setup
```yaml
# aws-deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-server
  template:
    metadata:
      labels:
        app: vllm-server
    spec:
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        ports:
        - containerPort: 8000
        env:
        - name: MODEL_NAME
          value: "Qwen/Qwen1.5-7B-Chat"
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "8Gi"
            cpu: "2"
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "4"
```

#### Azure Setup
```json
{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "vmName": {
      "type": "string",
      "defaultValue": "ivme-vllm-vm"
    }
  },
  "resources": [
    {
      "type": "Microsoft.Compute/virtualMachines",
      "apiVersion": "2021-03-01",
      "name": "[parameters('vmName')]",
      "location": "[resourceGroup().location]",
      "properties": {
        "hardwareProfile": {
          "vmSize": "Standard_NC6s_v3"
        },
        "osProfile": {
          "computerName": "[parameters('vmName')]",
          "adminUsername": "azureuser",
          "customData": "[base64(concat('#cloud-config\nruncmd:\n  - docker run -d -p 8000:8000 vllm/vllm-openai:latest'))]"
        }
      }
    }
  ]
}
```

---

## 3.7 Troubleshooting ve YaygÄ±n Sorunlar

### âŒ Kurulum SorunlarÄ±

#### Extension YÃ¼klenmiyor
```bash
# Problem: Extension yÃ¼klenmiyor
# Ã‡Ã¶zÃ¼m 1: VS Code cache temizle
rm -rf ~/.vscode/extensions
code --list-extensions

# Ã‡Ã¶zÃ¼m 2: YÃ¶netici yetkileri ile yÃ¼kle (Windows)
# PowerShell'i yÃ¶netici olarak aÃ§
code --install-extension baykar-ivme-2.2.7.vsix

# Ã‡Ã¶zÃ¼m 3: Manuel kurulum
# Extension dosyasÄ±nÄ± VS Code extensions klasÃ¶rÃ¼ne kopyala
cp baykar-ivme-2.2.7.vsix ~/.vscode/extensions/
```

#### vLLM BaÄŸlantÄ± SorunlarÄ±
```bash
# Problem: vLLM sunucusuna baÄŸlanÄ±lamÄ±yor
# Debug 1: Port kontrolÃ¼
netstat -an | grep 8000
lsof -i :8000

# Debug 2: Sunucu loglarÄ±
docker logs vllm-server
# veya
tail -f vllm.log

# Debug 3: Manuel test
curl -X GET http://localhost:8000/v1/models
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"Qwen/Qwen1.5-7B-Chat","messages":[{"role":"user","content":"Hello"}]}'

# Ã‡Ã¶zÃ¼m: Firewall ayarlarÄ±
sudo ufw allow 8000
# Windows'ta Windows Defender Firewall'da port aÃ§
```

#### Gemini API SorunlarÄ±
```typescript
// API anahtarÄ± test
async function testGeminiApiKey(apiKey: string): Promise<boolean> {
  try {
    const response = await fetch(
      `https://generativelanguage.googleapis.com/v1beta/models?key=${apiKey}`
    );
    
    if (!response.ok) {
      console.error('Gemini API Error:', response.statusText);
      return false;
    }
    
    const data = await response.json();
    console.log('Available models:', data.models?.length || 0);
    return true;
  } catch (error) {
    console.error('Gemini API Test Failed:', error);
    return false;
  }
}
```

### ğŸ” Diagnostic Tools

#### Extension Health Check
```typescript
// Extension saÄŸlÄ±k kontrolÃ¼
class ExtensionHealthCheck {
  async runDiagnostics(): Promise<DiagnosticReport> {
    const report = new DiagnosticReport();
    
    // 1. Extension durumu
    report.addCheck('extension_active', this.isExtensionActive());
    
    // 2. API servisleri
    report.addCheck('vllm_available', await this.checkVllmService());
    report.addCheck('gemini_available', await this.checkGeminiService());
    
    // 3. Dosya sistemi izinleri
    report.addCheck('fs_permissions', await this.checkFileSystemAccess());
    
    // 4. Workspace durumu
    report.addCheck('workspace_ready', this.checkWorkspaceSetup());
    
    // 5. Bellek kullanÄ±mÄ±
    report.addCheck('memory_usage', this.checkMemoryUsage());
    
    return report;
  }
  
  private async checkVllmService(): Promise<boolean> {
    try {
      const config = vscode.workspace.getConfiguration('baykar-ai-fixer');
      const baseUrl = config.get<string>('vllm.baseUrl');
      
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000);
      
      const response = await fetch(`${baseUrl}/v1/models`, {
        signal: controller.signal
      });
      
      clearTimeout(timeoutId);
      return response.ok;
    } catch {
      return false;
    }
  }
}
```

#### Automated Repair
```typescript
// Otomatik onarÄ±m sistemi
class AutoRepair {
  async repairExtension(): Promise<RepairResult> {
    const results: RepairResult[] = [];
    
    // 1. KonfigÃ¼rasyon onarÄ±mÄ±
    results.push(await this.repairConfiguration());
    
    // 2. Cache temizliÄŸi
    results.push(await this.clearCaches());
    
    // 3. Ä°ndeks yeniden oluÅŸturma
    results.push(await this.rebuildIndex());
    
    // 4. Servis baÄŸlantÄ± testleri
    results.push(await this.testServiceConnections());
    
    return new CombinedRepairResult(results);
  }
  
  private async repairConfiguration(): Promise<RepairResult> {
    try {
      const config = vscode.workspace.getConfiguration('baykar-ai-fixer');
      
      // Eksik ayarlarÄ± varsayÄ±lan deÄŸerlerle doldur
      const defaults = {
        'api.activeService': 'vLLM',
        'chat.temperature': 0.7,
        'chat.tokenLimit': 12000,
        'indexing.enabled': true
      };
      
      for (const [key, value] of Object.entries(defaults)) {
        if (config.get(key) === undefined) {
          await config.update(key, value, vscode.ConfigurationTarget.Global);
        }
      }
      
      return new RepairResult('configuration', true, 'KonfigÃ¼rasyon onarÄ±ldÄ±');
    } catch (error) {
      return new RepairResult('configuration', false, error.message);
    }
  }
}
```

---

<div align="center">
  <h2>âš¡ HÄ±zlÄ± ve Kolay Kurulum</h2>
  <p><em>BirkaÃ§ dakikada production-ready Ä°vme deneyimi</em></p>
</div>
